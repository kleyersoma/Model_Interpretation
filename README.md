# Model Interpretation

Model Interpretation is considered one of the most difficult tasks of the pipeline of a Machine Learning Project, in this repository
I try to bring new tools, that can help in explaining what a machine learning model learns in order to take a decissions, classification
or inference.

Interpreting machine learning and deep learning models is a required task in the data science lifecycle since data scientists or machine learning engineers 
are often involved with serving the models into production.

Interpreting a model is essencial to know how the **"black box"** is working, questions such as:
* How does a machine learning or deep learning model generates its decisions?
  * It is good to know the inner workings of the chosen model, the science tha makes the model work.
* Are those decission reliable?
  * This is the question that gets an aswer by the area of Explainable Artificial Intelligence.

Performance metrics are not the only way for evaluating how good or bad a model performs, the model must answer questions for fairnes, 
transparency and accountability for any model being used to solve real-world problems.

### Repository Structure:
* Learning from a Machine Learning Model Interpretation: This directory contains a notebook that intents to explain what a machine learning algorithm
  learns that defines its decissions, classifications or inferences. 
* What do Deep Learning Models for Computer Vision See?: The general objectives of this directory and the notebook in it is to explain 
  and use concepts, techniques and tools to interpret and understand deep learning models used in Computer Vision. 
  The architecture that is used is a Convolutional Neural Network. The Deep Learning Models will be implemented using Keras and Tensorflow 2.0 
  and some open-source tools to visually interpret decisions made by these models.
